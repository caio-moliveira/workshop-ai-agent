# ğŸ” 04-RAG - Retrieval-Augmented Generation

Esta pasta contÃ©m uma implementaÃ§Ã£o educacional completa de **Retrieval-Augmented Generation (RAG)**, demonstrando cada etapa do processo desde o carregamento de documentos atÃ© a geraÃ§Ã£o de respostas contextuais usando tanto **Ollama** (local) quanto **OpenAI**.

## ğŸ¯ Objetivo Educacional

Demonstrar na prÃ¡tica todos os conceitos fundamentais de RAG atravÃ©s de exemplos progressivos, desde conceitos bÃ¡sicos atÃ© implementaÃ§Ã£o completa, focando em:
- **Pipeline RAG completo** passo-a-passo
- **Chunking inteligente** e suas implicaÃ§Ãµes
- **Embeddings vetoriais** e busca semÃ¢ntica
- **AvaliaÃ§Ã£o de qualidade** com mÃ©tricas
- **OtimizaÃ§Ã£o de performance** e debugging

## ğŸ“ Estrutura dos Arquivos

### ğŸš€ Pipeline Completo
| Arquivo | DescriÃ§Ã£o | NÃ­vel |
|---------|-----------|-------|
| **`RAG_pipeline.py`** | Sistema RAG completo e funcional | â­â­â­ **Principal** |

### ğŸ“š Conceitos Detalhados (Step-by-Step)
| Arquivo | Conceito | Foco Educacional |
|---------|----------|------------------|
| **`chunking-example.py`** | Text Chunking | Como texto Ã© dividido e impactos |
| **`embedding-example.py`** | Vector Embeddings | Como texto vira nÃºmeros |
| **`semantic-search-example.py`** | Busca SemÃ¢ntica | Similaridade vs palavras-chave |
| **`context_enrichment.py`** | Context Enrichment | PreparaÃ§Ã£o para geraÃ§Ã£o |

### ğŸ“Š Dataset e AvaliaÃ§Ã£o
| Item | DescriÃ§Ã£o |
|------|-----------|
| **`data/Understanding_Climate_Change.pdf`** | Documento exemplo para demonstraÃ§Ãµes |
| **`db/`** | Bancos vetoriais (Chroma + FAISS) |

## ğŸ§© Pipeline RAG Detalhado

### ğŸ“– Step 1: Document Loading
```python
# Carregamento de PDF com metadados
loader = PyPDFLoader(file_path)
documents = loader.load()
```
**Conceitos:** Processamento de documentos, preservaÃ§Ã£o de metadados

### âœ‚ï¸ Step 2: Text Chunking
```python
# DivisÃ£o inteligente com overlap
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Tamanho do chunk
    chunk_overlap=200,    # SobreposiÃ§Ã£o para contexto
    separators=["\n\n", "\n", " ", ""]  # Hierarquia de separaÃ§Ã£o
)
```
**Conceitos:** Balanceamento contexto vs precisÃ£o, continuidade semÃ¢ntica

### ğŸ”¢ Step 3: Vector Embeddings
```python
# ConversÃ£o texto â†’ vetores
embeddings_model = OllamaEmbeddings(model="mxbai-embed-large")
vectorstore = Chroma.from_documents(documents, embeddings_model)
```
**Conceitos:** RepresentaÃ§Ã£o semÃ¢ntica, similaridade cosine

### ğŸ” Step 4: Semantic Search
```python
# Busca por similaridade
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}  # Top 5 resultados
)
```
**Conceitos:** RelevÃ¢ncia semÃ¢ntica vs sintÃ¡tica

### ğŸ§  Step 5: Context Enrichment
```python
# PreparaÃ§Ã£o do contexto
context = "\n\n".join([doc.page_content for doc in relevant_docs])
```
**Conceitos:** OtimizaÃ§Ã£o de context window

### ğŸ’¬ Step 6: Answer Generation
```python
# GeraÃ§Ã£o com contexto
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)
response = llm.invoke([SystemMessage(context), HumanMessage(query)])
```
**Conceitos:** Prompt engineering para RAG

## ğŸš€ Como Usar

### 1. ConfiguraÃ§Ã£o Inicial

```bash
# 1. Instalar Ollama (Opcional para uso local)
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull mxbai-embed-large:latest
ollama pull mistral:latest

# 2. Instalar dependÃªncias Python
pip install -r requirements.txt

# 3. Configurar APIs (opcional)
export OPENAI_API_KEY="sua-chave-openai"
export TAVILY_API_KEY="sua-chave-tavily"  # Para busca web
```

### 2. Executar Pipeline Completo

```bash
# RAG completo com interface interativa
python RAG_pipeline.py

# Escolher durante execuÃ§Ã£o:
# 1. Apenas Ollama (100% local, gratuito)
# 2. Ollama + OpenAI (embeddings locais, geraÃ§Ã£o OpenAI)
```

### 3. Explorar Conceitos Individuais

```bash
# Entender chunking
python chunking-example.py

# Ver embeddings em aÃ§Ã£o
python embedding-example.py

# Comparar busca semÃ¢ntica vs tradicional
python semantic-search-example.py

# Analisar enriquecimento de contexto
python context_enrichment.py
```

## ğŸ“Š Tecnologias e Modelos

### ğŸ§  Modelos de Embedding
| Modelo | DimensÃµes | Idioma | Velocidade | Qualidade |
|--------|-----------|--------|------------|-----------|
| **mxbai-embed-large** | 1024 | Multilingual | ğŸš€ RÃ¡pido | â­â­â­â­â­ |
| **text-embedding-3-small** | 1536 | Multilingual | ğŸŒ API | â­â­â­â­ |

### ğŸ—„ï¸ Vector Stores
| Store | Uso | Performance | PersistÃªncia |
|-------|-----|-------------|--------------|
| **Chroma** | Desenvolvimento | ğŸš€ RÃ¡pido | âœ… Local |
| **FAISS** | ProduÃ§Ã£o | âš¡ Ultra-rÃ¡pido | âœ… Local |

### ğŸ¤– LLMs Suportados
- **Ollama Local**: mistral, llama2, deepseek-r1
- **OpenAI**: gpt-3.5-turbo, gpt-4
- **ConfigurÃ¡vel**: Temperatura, max_tokens, etc.

## âš™ï¸ ParÃ¢metros de OtimizaÃ§Ã£o

### ğŸ“ Chunking Strategy
```python
# ConfiguraÃ§Ã£o recomendada
chunk_size = 1000        # BalanÃ§o contexto/precisÃ£o
chunk_overlap = 200      # 20% overlap para continuidade
separators = ["\n\n", "\n", " "]  # HierÃ¡rquica
```

### ğŸ” Retrieval Configuration
```python
# Busca otimizada
search_type = "similarity"    # vs "mmr" (diversidade)
k = 5                        # NÃºmero de chunks
score_threshold = 0.7        # Filtro de relevÃ¢ncia
```

### ğŸ›ï¸ Generation Settings
```python
# LLM para geraÃ§Ã£o
temperature = 0.3            # BalanÃ§o precisÃ£o/criatividade
max_tokens = 1000           # Limite de resposta
top_p = 0.9                 # Nucleus sampling
```

## ğŸ“ˆ AnÃ¡lise de Performance

### ğŸ¯ MÃ©tricas de Qualidade

**Retrieval Metrics:**
- **Precision@k**: RelevÃ¢ncia dos top-k chunks
- **Recall@k**: Cobertura de informaÃ§Ã£o relevante
- **MRR**: Mean Reciprocal Rank

**Generation Metrics:**
- **Faithfulness**: Fidelidade ao contexto recuperado
- **Relevancy**: RelevÃ¢ncia contextual da resposta
- **Correctness**: PrecisÃ£o factual

### ğŸ”§ Debugging RAG

**Problemas Comuns e SoluÃ§Ãµes:**

1. **âŒ Chunks Irrelevantes**
   ```python
   # SoluÃ§Ã£o: Ajustar chunking
   chunk_size = 1500  # Mais contexto
   chunk_overlap = 300  # Mais continuidade
   ```

2. **âŒ Contexto Insuficiente** 
   ```python
   # SoluÃ§Ã£o: Mais chunks
   search_kwargs = {"k": 8}  # Mais resultados
   ```

3. **âŒ Respostas Inconsistentes**
   ```python
   # SoluÃ§Ã£o: Menor temperatura
   temperature = 0.1  # Mais determinÃ­stico
   ```

## ğŸ’¡ Casos de Uso Ideais

### âœ… Ã“timo para RAG
- **ğŸ“š DocumentaÃ§Ã£o tÃ©cnica**: APIs, manuais, guias
- **ğŸ¢ Base de conhecimento**: FAQ, polÃ­ticas, procedimentos  
- **ğŸ”¬ Pesquisa acadÃªmica**: Papers, relatÃ³rios, estudos
- **ğŸ’¼ ConteÃºdo empresarial**: RelatÃ³rios, apresentaÃ§Ãµes

### âš ï¸ LimitaÃ§Ãµes do RAG
- **ğŸ“… InformaÃ§Ãµes desatualizadas**: RAG Ã© estÃ¡tico
- **ğŸ§® RaciocÃ­nio complexo**: Melhor usar fine-tuning
- **ğŸ“Š Dados estruturados**: Considerar SQL/GraphRAG
- **â° Tempo real**: InformaÃ§Ãµes dinÃ¢micas

## ğŸ§ª Experimentos e ExtensÃµes

### ğŸ”¬ TÃ©cnicas AvanÃ§adas (Para Estudo)

1. **Advanced Chunking**:
   - Semantic chunking (por tÃ³picos)
   - Document-aware splitting
   - Hierarchical chunking

2. **Retrieval Enhancement**:
   - Hybrid search (semÃ¢ntica + palavra-chave)
   - Reranking models
   - Query expansion/reformulation

3. **Context Optimization**:
   - Context compression
   - Relevant sentence extraction
   - Multi-hop reasoning

4. **Evaluation Framework**:
   - Human evaluation
   - A/B testing
   - Domain-specific metrics

## ğŸ”— IntegraÃ§Ã£o com Outros MÃ³dulos

| MÃ³dulo | IntegraÃ§Ã£o RAG |
|--------|----------------|
| **02-frameworks** | RAG como ferramenta para agentes |
| **03-prompt-engineering** | OtimizaÃ§Ã£o de prompts RAG |
| **05-memory** | RAG como sistema de memÃ³ria externa |
| **06-MCP-A2A** | RAG via Model Context Protocol |

## ğŸ“š Conceitos Demonstrados

### ğŸ“ Para Estudantes
- **Vector databases**: Armazenamento e busca eficiente
- **Embedding models**: RepresentaÃ§Ã£o semÃ¢ntica de texto
- **Similarity search**: RecuperaÃ§Ã£o por significado
- **Context window management**: OtimizaÃ§Ã£o de prompt
- **Local vs Cloud**: Trade-offs Ollama vs OpenAI
- **Evaluation frameworks**: MÃ©tricas de qualidade

### ğŸ”¬ Para Pesquisadores
- **Information retrieval**: TÃ©cnicas de recuperaÃ§Ã£o
- **Neural embeddings**: RepresentaÃ§Ãµes distribuÃ­das
- **Prompt engineering**: OtimizaÃ§Ã£o de instruÃ§Ãµes
- **Performance optimization**: Tuning de hiperparÃ¢metros

## ğŸ† Resultados Esperados

ApÃ³s completar este mÃ³dulo, vocÃª serÃ¡ capaz de:

âœ… **Implementar** um sistema RAG completo  
âœ… **Otimizar** parÃ¢metros para diferentes casos de uso  
âœ… **Avaliar** qualidade usando mÃ©tricas apropriadas  
âœ… **Debuggar** problemas comuns em RAG  
âœ… **Escolher** entre soluÃ§Ãµes locais vs cloud  
âœ… **Integrar** RAG em sistemas maiores  

---

**ğŸ¯ PrÃ³ximos Passos:** Explore **06-MCP-A2A** para ver como integrar RAG em arquiteturas multi-agente!